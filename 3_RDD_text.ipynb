{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WjAogjR3exK",
        "outputId": "9807ce6c-7787-4be9-d8bd-bee5f9d8beab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285397 sha256=6ff2e5b08037db0c0149d713c0a3d8af5d97d857c7ec5326cf92d11b80fffb95\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-ci4Upm3jr2",
        "outputId": "86664d2b-c966-450f-93d1-5325a2afa2fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "elements = 6\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "spark = SparkContext.getOrCreate()\n",
        "data = [\"scala\",\"java\",\"hadoop\",\"spark\",\"pyspark\",\"spark vs hadoop\"]\n",
        "rdd = spark.parallelize(data)\n",
        "counts = rdd.count()\n",
        "print(\"elements = %d\"% (counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_2rWDon5IMY",
        "outputId": "122a0813-5b4f-43fe-d0db-977261770fa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elements of rdd = ['scala', 'java', 'hadoop', 'spark', 'pyspark', 'spark vs hadoop']\n"
          ]
        }
      ],
      "source": [
        "collect = rdd.collect()\n",
        "print(\"Elements of rdd = %s\"%(collect))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_buZxN4E6BLg"
      },
      "outputs": [],
      "source": [
        "#for each loop\n",
        "\n",
        "rdd = spark.parallelize(\n",
        "    [\"scala\", \"java\",\"python\",\"R\",\"pyspark\",\"hadoop\"]\n",
        "\n",
        ")\n",
        "def f(x):\n",
        "  print(x)\n",
        "\n",
        "rdd.foreach(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-PJC7iP6BTW",
        "outputId": "8f04fcc4-803b-493d-b376-b7e0954af8da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "elements = ['spark', 'pyspark', 'spark vs hadoop']\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "spark = SparkContext.getOrCreate()\n",
        "data = [\"scala\",\"java\",\"hadoop\",\"spark\",\"pyspark\",\"spark vs hadoop\"]\n",
        "rdd = spark.parallelize(data)\n",
        "data_filter = rdd.filter(lambda x: 'spark' in x)\n",
        "filtered = data_filter.collect()\n",
        "print(\"elements = %s\"% (filtered))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TbQpOOw6BXA",
        "outputId": "c9af6c70-b556-4d20-ef8a-f5c7bcd0c06d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('scala', 1), ('java', 1), ('python', 1), ('R', 1), ('pyspark', 1), ('hadoop', 1)]\n",
            "key value pair = [('scala', 1), ('java', 1), ('python', 1), ('R', 1), ('pyspark', 1), ('hadoop', 1)]\n"
          ]
        }
      ],
      "source": [
        "#mapping\n",
        "rdd = spark.parallelize(\n",
        "    [\"scala\", \"java\",\"python\",\"R\",\"pyspark\",\"hadoop\"]\n",
        "\n",
        ")\n",
        "word_map = rdd.map(lambda x: (x,1))\n",
        "mapping = word_map.collect()\n",
        "print(mapping)\n",
        "print(\"key value pair = %s\"%(mapping))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1JRu5Tk6BaO"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContent\n",
        "from operator import add\n",
        "sc= SparkContent.getOrCreate()\n",
        "rdd = sc.parallelize([1,2,3,4,5])\n",
        "add = rdd.reduce(add)\n",
        "print(\"Adding elements -> %i\" % (sum))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6TtMvW86BdW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRtZvJS86Bgo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3OlxRGY6BkI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c2QZK_n6Bnn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kl1X_dTp6BrO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-_y2dYK6Bu2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD8fFUJx6Bye"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0U1U_gd6B4X"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
